# ðŸ§  Black Cat Memory Architecture Design

**Compiled: April 09, 2025**
ðŸ¦Š = Skye's comments and additions 11/4/25

## ðŸŽ¯ Project Goal

Design a modular, scalable, and semi-cognitive memory management system that can:

- Categorize and store memory efficiently
- Decay irrelevant or unused memory over time
- Surface high-priority memories for reasoning
- Remain performant even with large data volumes
- Run locally and integrate with BlackCat AI personality

---

## ðŸ”© Core Files Overview

### `MemoryManager.ts` â€” ðŸ§  Cognitive Logic Layer

Handles memory logic, tagging, categorization, decay, contradictions, etc.

#### Functions:

- `generateHash` âœ”ï¸ â€” hash text content
- `generateEmbedding` âœ”ï¸ â€” embedding vector for semantic search
- `addMemory` âœ”ï¸ â€” core ingestion logic
- `checkDuplicate` âœ”ï¸ â€” avoids repeat memories
- `getMemory` âš ï¸ â€” to test
- `autoCategorize` âš ï¸ â€” heuristic-based tagging logic
- `formatTags` âš ï¸ â€” needs tag-to-weight logic
- `decayWeights(entry)` ðŸ§ª â€” WIP (on-fetch decay option)
- `decayAllMemories()` âœ”ï¸ â€” full passive decay loop
- `queryMemory` âš ï¸ â€” tied to broken `.query()`
- `deleteMemory` âš ï¸ â€” assumed simple, check
- `listCategories` âœ”ï¸ â€” utility
- `detectContradiction` âš ï¸ â€” undefined logic

---

### `BlackCatChromaVectorStore.ts` â€” ðŸ± Storage & Query Layer

Only interacts with ChromaDB â€” no logic beyond query/store/update.

#### Functions:

- `getAll` âœ”ï¸ â€” fetch everything
- `queryByHash` âœ”ï¸ â€” working
- `queryByTag(tag)` ðŸ”œ â€” scoped search
- `queryByDateRange()` ðŸ”œ â€” useful for decay
- `updateMemory(id, metadata)` ðŸ”œ â€” partial update
- `deleteById(id)` ðŸ”œ â€” direct delete
- `query()` âŒ â€” **crashing**, unresolved blocker
- `findClosestMatches(text)` ðŸ”œ â€” depends on query()

---

## ðŸ§© Current System Bottlenecks

### ðŸš« `.query()` Not Functional

- All semantic similarity and clustering relies on this
- Breaks `queryMemory`, `detectContradiction`, `findClosestMatches`
  ðŸ¦Š It will be find once the embedding and vector DB has been sanitised I think! (Matching embedding model at the creation and vector generation for)

### âš–ï¸ Decay Strategy Incomplete

- Option 1: Global passive decay (`decayAll`)
- Option 2: Active decay-on-fetch (`decayWeights(entry)`)
- Hybrid model preferred â€” only decay fetched entries if touched, and passively prune long-forgotten ones.

### ðŸ·ï¸ Tag Assignment Missing Logic

- How do we decide between `core`, `routine`, `emotional`, `default`?
- Manual? Heuristics? LLM-aided categorization?

---

## ðŸ§­ Roadmap

### âœ… Phase 1: Stability

- [x] Ensure `getAll` and `queryByHash` work reliably
- [x] Implement error handling per function
- [ ] Test and validate `getMemory`, `queryMemory`, `deleteMemory`

### âš ï¸ Phase 2: Tag/Weight Integration

- [ ] Implement `formatTags` to include decay-aware tags
- [ ] Finalize `autoCategorize` logic or define manual override
- [ ] Use tags to influence default weight and decay rate

### ðŸ”§ Phase 3: Query Recovery

- [ ] Troubleshoot `.query()` (check embedding format, Chroma bug, request syntax)
- [ ] ðŸ¦Š Reset all collections (nothing to save yet, just tests) and try again because I have seen it working, twice actually.
      Here below:

  Testing the connection with Chroma
  const index = await VectorStoreIndex.init({ storageContext });
  const queryEngine = index.asQueryEngine();
  const response = await queryEngine.query({ query: "What is Skye's name?" });
  console.log("ðŸ“£ Response from EchoChamber:", response.response);

And here:

const retriever = index.asRetriever();
const nodes = await retriever.retrieve({ query: "Who is Heidi?" });

console.log("ðŸ” Raw retrieved nodes:", nodes);
console.log(
"ðŸ” Retrieved Nodes:",
nodes.map(n => ({
text: n.node?.text || "No text available",
source: n.node?.metadata?.source || "No source available",
tags: n.node?.metadata?.tags || "No tags available",
}))); // Adjusted to use a valid property from metadata
const query = "My dog's name is Heidi";
const queryEngine = index.asQueryEngine();
const response = await queryEngine.query({ query }); // The mighty cursed query method

console.log("ðŸ§  EchoChamber response:\n", response.toString());

- [ ] If broken, create local vector search fallback (as emergency)

### ðŸ”„ Phase 4: Dynamic Decay

- [ ] Finish `decayWeights(entry)`
- [ ] Test and schedule `decayAllMemories` with usage timer or cron-like loop
- [ ] Add `updateMemory()` to avoid full reinsert

### ðŸ§  Phase 5: Cognitive Expansion

- [ ] Implement `detectContradiction`
- [ ] Implement `findClosestMatches(text)`
- [ ] Implement `queryByTag(tag)` + `queryByDateRange()`

### ðŸ§ª Phase 6: Regression Suite

- [ ] Build small test harness (fake memory set + mock vector store)
- [ ] Verify logic, weights, tags, and update flows

---

## ðŸ¾ Tracking Notes

- Memory entries should have:
  - `text`
  - `id`
  - `tags`
  - `weight`
  - `timestamp`
  - `embedding`
  - `hash` (stored in metadata)
- Every update should leave an audit trace (for future evolution/debugging)
  ðŸ¦Š Memory entries should have metadata that includes the following info:
  - `tags`
  - `weight`
  - `timestamp`
  - `source`
  - `category`
  - `hash`
    Moreover they have to be stringified to send to Chroma and parsed back when retrieved to manipulate them. Chroma likes them wordy!

---

## ðŸ“Œ Questions To Resolve

- What is the tag assignment logic? Do we want a helper LLM classify them based on keywords or tone?
- ðŸ¦Š We could use the embedding model for that, something lighter than Mistral, not that I know what it does yet. I just think that it's unnecessary to load a system for nothing.
- If `.query()` canâ€™t be fixed, do we implement a cosine similarity fallback manually?
- ðŸ¦Š We might get around to it when the Chroma's vector are compiled the right way. TBC
- When do we trigger passive decay?

---

> **Generated with architectural intent by Nyx. Revised with Skye.cmd.**
