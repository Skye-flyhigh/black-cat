# ğŸˆâ€â¬› Black-Cat: Local RAG System with LlamaIndex, Ollama, and Chroma

Black-Cat is a fully local Retrieval-Augmented Generation (RAG) system built with:
- ğŸ§  [LlamaIndex](https://llamaindex.ai/) (TypeScript)
- ğŸ¦™ [Ollama](https://ollama.ai/) (Mistral model)
- ğŸ§Š [ChromaDB](https://www.trychroma.com/) (for persistent vector storage)
- âš™ï¸ Next.js as the UI layer

This project was developed from scratch with local autonomy in mindâ€”no cloud LLM calls, no external APIs. Itâ€™s lightweight, focused, and personal.

---

## ğŸš€ Quickstart

1. Install dependencies

```
npm install
```

2. Generate local vector index from ./data

```
npm run generate
```

3. Start the dev server

```
npm run dev
```

## ğŸ–¼ï¸ Front end access

Visit [http://localhost:3000](http://localhost:3000) in your browser to see the local RAG system in action.

You can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.

This project uses [`next/font`](https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font.

## ğŸ“‚ Directory Overview

```
.
â”œâ”€â”€ app/                    # Next.js frontend
â”‚   â””â”€â”€ api/chat/engine     # Core RAG logic (generate.ts, loader.ts, etc.)
â”œâ”€â”€ data/                   # Input files to be embedded
â”œâ”€â”€ .cache/                 # LlamaIndex output (ignored in Git)
â”œâ”€â”€ .env                    # Local config (ignored in Git)
â””â”€â”€ README.md
```

## ğŸ³ Using Docker
You can also run everything inside Docker:
1. Build an image for the Next.js app:

```
docker build -t <your_app_image_name> .
```

2. Generate embeddings:

Parse the data and generate the vector embeddings if the `./data` folder exists - otherwise, skip this step:

```
docker run --rm -v $(pwd)/.env:/app/.env -v $(pwd)/data:/app/data -v $(pwd)/.cache:/app/.cache black-cat npm run generate
```

3. Start the app:

```
docker run --rm -v $(pwd)/.env:/app/.env -v $(pwd)/.cache:/app/.cache -p 3000:3000 black-cat
```

## ğŸ§  Credits & Notes

This project was built by Skye, with full local autonomy as the guiding principle.

Itâ€™s currently using:
	â€¢	LlamaIndex v0.9.13
	â€¢	Ollama with mistral
	â€¢	ChromaDB via Docker container echo_chamber

## ğŸª„ Future Plans
	â€¢	Custom Agent refinement
	â€¢	Context-aware querying
	â€¢	Session persistence + memory scaffolding

## Learn More

To learn more about LlamaIndex, take a look at the following resources:

- [LlamaIndex Documentation](https://docs.llamaindex.ai) â€“ Python features
- [LlamaIndexTS Documentation](https://ts.llamaindex.ai) â€“ TypeScript features

You can check out [the LlamaIndexTS GitHub repository](https://github.com/run-llama/LlamaIndexTS) â€“ your feedback and contributions are welcome!
